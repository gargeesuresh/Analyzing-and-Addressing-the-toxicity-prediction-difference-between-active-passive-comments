{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-label Text Classification with BERT and PyTorch Lightning","metadata":{"id":"ePVSga5E_a-e"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"EWKsmCRIqMCW","outputId":"86bb24eb-6e5f-417a-82d5-0ba17e7b3269","execution":{"iopub.status.busy":"2021-12-16T18:39:19.407299Z","iopub.execute_input":"2021-12-16T18:39:19.407915Z","iopub.status.idle":"2021-12-16T18:39:20.219856Z","shell.execute_reply.started":"2021-12-16T18:39:19.407881Z","shell.execute_reply":"2021-12-16T18:39:20.218865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-lightning --quiet\n!pip install transformers --quiet","metadata":{"id":"f3tx1knExlud","execution":{"iopub.status.busy":"2021-12-17T15:27:08.205049Z","iopub.execute_input":"2021-12-17T15:27:08.205622Z","iopub.status.idle":"2021-12-17T15:27:24.572927Z","shell.execute_reply.started":"2021-12-17T15:27:08.205503Z","shell.execute_reply":"2021-12-17T15:27:24.572132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.metrics.functional import accuracy, f1, auroc\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nRANDOM_SEED = 42\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\npl.seed_everything(RANDOM_SEED)","metadata":{"id":"y5mj8cpdM_O3","outputId":"6129f9c6-a8ea-4358-dfef-712e1ae287b7","execution":{"iopub.status.busy":"2021-12-18T02:41:20.767573Z","iopub.execute_input":"2021-12-18T02:41:20.76813Z","iopub.status.idle":"2021-12-18T02:41:29.196196Z","shell.execute_reply.started":"2021-12-18T02:41:20.768024Z","shell.execute_reply":"2021-12-18T02:41:29.19544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\n\nOur dataset contains potentially offensive (toxic) comments and comes from the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).","metadata":{"id":"btZjckeJs5j7"}},{"cell_type":"code","source":"!unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n","metadata":{"id":"zpT48Rwl94aP","outputId":"d4d8e244-e0bf-4c84-88ae-d569467357a5","execution":{"iopub.status.busy":"2021-12-18T02:41:30.774298Z","iopub.execute_input":"2021-12-18T02:41:30.774602Z","iopub.status.idle":"2021-12-18T02:41:32.189458Z","shell.execute_reply.started":"2021-12-18T02:41:30.774565Z","shell.execute_reply":"2021-12-18T02:41:32.1886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load and look at the data:","metadata":{"id":"FEQ-aGlT5yTU"}},{"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\ndf.head()","metadata":{"id":"6v7DW58wrm-w","outputId":"5068e774-bc13-4a8f-8f62-b0ed4b3dc369","execution":{"iopub.status.busy":"2021-12-18T02:41:35.594161Z","iopub.execute_input":"2021-12-18T02:41:35.594988Z","iopub.status.idle":"2021-12-18T02:41:36.545288Z","shell.execute_reply.started":"2021-12-18T02:41:35.594934Z","shell.execute_reply":"2021-12-18T02:41:36.544453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have text (comment) and six different toxic labels. Note that we have clean content, too. \n\nLet's split the data:","metadata":{"id":"hX26ij7N6Uv_"}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-18T02:41:49.336033Z","iopub.execute_input":"2021-12-18T02:41:49.336291Z","iopub.status.idle":"2021-12-18T02:41:49.341448Z","shell.execute_reply.started":"2021-12-18T02:41:49.336261Z","shell.execute_reply":"2021-12-18T02:41:49.340656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.05)\ntrain_df.shape, val_df.shape","metadata":{"id":"1cOIF3fD5r_8","outputId":"a04aa3ea-8d59-4f7d-d8c0-ada662ac9ba2","execution":{"iopub.status.busy":"2021-12-18T02:49:42.504995Z","iopub.execute_input":"2021-12-18T02:49:42.505252Z","iopub.status.idle":"2021-12-18T02:49:42.542555Z","shell.execute_reply.started":"2021-12-18T02:49:42.505221Z","shell.execute_reply":"2021-12-18T02:49:42.541736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing and Visualisation\n\nLet's look at the distribution of the labels:","metadata":{"id":"EU9fW3KD7V0V"}},{"cell_type":"code","source":"LABEL_COLUMNS = df.columns.tolist()[2:]\ndf[LABEL_COLUMNS].sum().sort_values().plot(kind=\"barh\");","metadata":{"id":"qz9HXNwL7_1b","outputId":"3218d794-735d-497e-c001-50daae8f683b","execution":{"iopub.status.busy":"2021-12-18T02:49:49.940095Z","iopub.execute_input":"2021-12-18T02:49:49.940358Z","iopub.status.idle":"2021-12-18T02:49:50.372559Z","shell.execute_reply.started":"2021-12-18T02:49:49.94033Z","shell.execute_reply":"2021-12-18T02:49:50.370995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a severe case of imbalance. But that is not the full picture. What about the toxic vs clean comments?","metadata":{"id":"cC3Z4RTZ9T-L"}},{"cell_type":"code","source":"train_toxic = train_df[train_df[LABEL_COLUMNS].sum(axis=1) > 0]\ntrain_clean = train_df[train_df[LABEL_COLUMNS].sum(axis=1) == 0]\n\npd.DataFrame(dict(\n  toxic=[len(train_toxic)], \n  clean=[len(train_clean)]\n)).plot(kind='barh');","metadata":{"id":"acF4YRlQL8iz","outputId":"2f2fe334-f620-42a9-9b58-ce15fd70d818","execution":{"iopub.status.busy":"2021-12-18T02:49:53.364097Z","iopub.execute_input":"2021-12-18T02:49:53.364395Z","iopub.status.idle":"2021-12-18T02:49:53.736712Z","shell.execute_reply.started":"2021-12-18T02:49:53.364363Z","shell.execute_reply":"2021-12-18T02:49:53.736007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we have a severe imbalance in favor of the clean comments. To combat this, we'll sample 15,000 examples from the clean comments and create a new training set:","metadata":{"id":"vWI3QUF6E1Bp"}},{"cell_type":"code","source":"train_df = pd.concat([\n  train_toxic.sample(8000),\n  train_clean.sample(8000)\n])\n\ntrain_df.shape, val_df.shape","metadata":{"id":"4_dLeMh4ZJx9","outputId":"3ee0e292-d268-4dc6-fde7-d11511624622","execution":{"iopub.status.busy":"2021-12-18T02:49:56.907077Z","iopub.execute_input":"2021-12-18T02:49:56.907611Z","iopub.status.idle":"2021-12-18T02:49:56.931103Z","shell.execute_reply.started":"2021-12-18T02:49:56.907574Z","shell.execute_reply":"2021-12-18T02:49:56.930353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization\n\nWe need to convert the raw text into a list of tokens. For that, we'll use the built-in BertTokenizer:","metadata":{"id":"EwBpI_G0FjwO"}},{"cell_type":"code","source":"BERT_MODEL_NAME = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)","metadata":{"id":"fU8CYzKonYft","outputId":"40848da0-45c3-47a5-b95d-f3e18cf5f1cb","execution":{"iopub.status.busy":"2021-12-18T02:50:00.442239Z","iopub.execute_input":"2021-12-18T02:50:00.442502Z","iopub.status.idle":"2021-12-18T02:50:01.973499Z","shell.execute_reply.started":"2021-12-18T02:50:00.442473Z","shell.execute_reply":"2021-12-18T02:50:01.972804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try it out on a sample comment:","metadata":{"id":"jb6VLAmGHeEA"}},{"cell_type":"code","source":"sample_row = df.iloc[16]\nsample_comment = sample_row.comment_text\nsample_labels = sample_row[LABEL_COLUMNS]\n\nprint(sample_comment)\nprint()\nprint(sample_labels.to_dict())","metadata":{"id":"3kDTQ5_6RnkI","outputId":"dbc10141-1e5f-47cd-ffc0-ca9106a9b085","execution":{"iopub.status.busy":"2021-12-18T02:50:03.544688Z","iopub.execute_input":"2021-12-18T02:50:03.546725Z","iopub.status.idle":"2021-12-18T02:50:03.554183Z","shell.execute_reply.started":"2021-12-18T02:50:03.546669Z","shell.execute_reply":"2021-12-18T02:50:03.553053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  sample_comment,\n  add_special_tokens=True,\n  max_length=512,\n  return_token_type_ids=False,\n  padding=\"max_length\",\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n\nencoding.keys()","metadata":{"id":"DjLOQUgzUexM","outputId":"03c20ed5-28ac-4783-8781-6f1c0d5ee8d6","execution":{"iopub.status.busy":"2021-12-18T02:50:05.136029Z","iopub.execute_input":"2021-12-18T02:50:05.136622Z","iopub.status.idle":"2021-12-18T02:50:05.153596Z","shell.execute_reply.started":"2021-12-18T02:50:05.136586Z","shell.execute_reply":"2021-12-18T02:50:05.152874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding[\"input_ids\"].shape, encoding[\"attention_mask\"].shape","metadata":{"id":"t2Qoj-RJU_KR","outputId":"b02473a9-a206-4769-9f26-848da9494721","execution":{"iopub.status.busy":"2021-12-18T02:50:07.152062Z","iopub.execute_input":"2021-12-18T02:50:07.152602Z","iopub.status.idle":"2021-12-18T02:50:07.157869Z","shell.execute_reply.started":"2021-12-18T02:50:07.152563Z","shell.execute_reply":"2021-12-18T02:50:07.157193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result of the encoding is a dictionary with token ids `input_ids` and an attention mask `attention_mask` (which tokens should be used by the model 1 - use or 0 - don't use).\n\nLet's look at their contents:","metadata":{"id":"hfjz0npA3cVR"}},{"cell_type":"code","source":"encoding[\"input_ids\"].squeeze()[:20]","metadata":{"id":"iIO3LTJ1VK4S","outputId":"0fbb827d-07bb-4659-e9b3-685832d2e748","execution":{"iopub.status.busy":"2021-12-18T02:50:34.081589Z","iopub.execute_input":"2021-12-18T02:50:34.081887Z","iopub.status.idle":"2021-12-18T02:50:34.094193Z","shell.execute_reply.started":"2021-12-18T02:50:34.081855Z","shell.execute_reply":"2021-12-18T02:50:34.093375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding[\"attention_mask\"].squeeze()[:20]","metadata":{"id":"uCzsnq3eVQRj","outputId":"ed19f6f6-72f0-42f2-9221-bb9904e08c6a","execution":{"iopub.status.busy":"2021-12-18T02:50:35.648715Z","iopub.execute_input":"2021-12-18T02:50:35.649246Z","iopub.status.idle":"2021-12-18T02:50:35.656901Z","shell.execute_reply.started":"2021-12-18T02:50:35.649207Z","shell.execute_reply":"2021-12-18T02:50:35.65587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also inverse the tokenization and get back (kinda) the words from the token ids:","metadata":{"id":"dVZzFKOr-DXn"}},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())[:20])","metadata":{"id":"ib74R7MVsjmS","outputId":"4c36e133-2000-427c-bec2-f19c0685835e","execution":{"iopub.status.busy":"2021-12-17T15:41:41.959792Z","iopub.execute_input":"2021-12-17T15:41:41.96043Z","iopub.status.idle":"2021-12-17T15:41:41.967983Z","shell.execute_reply.started":"2021-12-17T15:41:41.960386Z","shell.execute_reply":"2021-12-17T15:41:41.967053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to specify the maximum number of tokens when encoding (512 is the maximum we can do). Let's check the number of tokens per comment:","metadata":{"id":"YALfbA2qKNp7"}},{"cell_type":"code","source":"token_counts = []\n\nfor _, row in train_df.iterrows():\n  token_count = len(tokenizer.encode(\n    row[\"comment_text\"], \n    max_length=512, \n    truncation=True\n  ))\n  token_counts.append(token_count)","metadata":{"id":"ymXBT82pIlpy","execution":{"iopub.status.busy":"2021-12-17T15:41:44.507367Z","iopub.execute_input":"2021-12-17T15:41:44.507907Z","iopub.status.idle":"2021-12-17T15:41:50.565154Z","shell.execute_reply.started":"2021-12-17T15:41:44.507869Z","shell.execute_reply":"2021-12-17T15:41:50.56445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(token_counts)\nplt.xlim([0, 512]);","metadata":{"id":"Lm8tHSuhJWNM","outputId":"b7c77053-ddbb-420f-be7e-10d6a5ca3cba","execution":{"iopub.status.busy":"2021-12-17T15:41:53.108501Z","iopub.execute_input":"2021-12-17T15:41:53.10883Z","iopub.status.idle":"2021-12-17T15:41:53.8301Z","shell.execute_reply.started":"2021-12-17T15:41:53.108781Z","shell.execute_reply":"2021-12-17T15:41:53.829412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the comments contain less than 300 tokens or more than 512. So, we'll stick with the limit of 512.","metadata":{"id":"vJmA8j2l6a_M"}},{"cell_type":"code","source":"MAX_TOKEN_COUNT = 512","metadata":{"id":"D63Sg9SzejIZ","execution":{"iopub.status.busy":"2021-12-17T15:42:22.596291Z","iopub.execute_input":"2021-12-17T15:42:22.596546Z","iopub.status.idle":"2021-12-17T15:42:22.600244Z","shell.execute_reply.started":"2021-12-17T15:42:22.596517Z","shell.execute_reply":"2021-12-17T15:42:22.599559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n\nWe'll wrap the tokenization process in a PyTorch Dataset, along with converting the labels to tensors:","metadata":{"id":"cgXNPSXU77oc"}},{"cell_type":"code","source":"class ToxicCommentsDataset(Dataset):\n\n  def __init__(\n    self, \n    data: pd.DataFrame, \n    tokenizer: BertTokenizer, \n    max_token_len: int = 128\n  ):\n    self.tokenizer = tokenizer\n    self.data = data\n    self.max_token_len = max_token_len\n    \n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self, index: int):\n    data_row = self.data.iloc[index]\n\n    comment_text = data_row.comment_text\n    labels = data_row[LABEL_COLUMNS]\n\n    encoding = self.tokenizer.encode_plus(\n      comment_text,\n      add_special_tokens=True,\n      max_length=self.max_token_len,\n      return_token_type_ids=False,\n      padding=\"max_length\",\n      truncation=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n\n    return dict(\n      comment_text=comment_text,\n      input_ids=encoding[\"input_ids\"].flatten(),\n      attention_mask=encoding[\"attention_mask\"].flatten(),\n      labels=torch.FloatTensor(labels)\n    )","metadata":{"id":"CQ1YdjRlYAxF","execution":{"iopub.status.busy":"2021-12-17T15:53:05.666857Z","iopub.execute_input":"2021-12-17T15:53:05.667117Z","iopub.status.idle":"2021-12-17T15:53:05.675605Z","shell.execute_reply.started":"2021-12-17T15:53:05.667087Z","shell.execute_reply":"2021-12-17T15:53:05.674651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at a sample item from the dataset:","metadata":{"id":"_nyqOozs9anR"}},{"cell_type":"code","source":"train_dataset = ToxicCommentsDataset(\n  train_df,\n  tokenizer,\n  max_token_len=MAX_TOKEN_COUNT\n)\n\nsample_item = train_dataset[0]\nsample_item.keys()","metadata":{"id":"M72dsvC-CDZD","outputId":"15d3eefa-ec2b-4cb8-d38c-ba42496fd4b6","execution":{"iopub.status.busy":"2021-12-17T15:53:06.551705Z","iopub.execute_input":"2021-12-17T15:53:06.552236Z","iopub.status.idle":"2021-12-17T15:53:06.563014Z","shell.execute_reply.started":"2021-12-17T15:53:06.552199Z","shell.execute_reply":"2021-12-17T15:53:06.562035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_item[\"comment_text\"]","metadata":{"id":"R6L8exQSiYM-","outputId":"86e24e42-008b-4907-d162-e787d4c02432","execution":{"iopub.status.busy":"2021-12-17T15:53:08.146716Z","iopub.execute_input":"2021-12-17T15:53:08.147295Z","iopub.status.idle":"2021-12-17T15:53:08.155725Z","shell.execute_reply.started":"2021-12-17T15:53:08.147256Z","shell.execute_reply":"2021-12-17T15:53:08.15485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_item[\"labels\"]","metadata":{"id":"ncSfaKkqiaFB","outputId":"2bb2a2ec-9770-4e02-fe88-a12a084da9c2","execution":{"iopub.status.busy":"2021-12-17T15:53:09.705562Z","iopub.execute_input":"2021-12-17T15:53:09.706039Z","iopub.status.idle":"2021-12-17T15:53:09.714346Z","shell.execute_reply.started":"2021-12-17T15:53:09.706002Z","shell.execute_reply":"2021-12-17T15:53:09.71355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_item[\"input_ids\"].shape","metadata":{"id":"Ab-Kr5Js499T","outputId":"c041bad2-7824-475b-f07e-85d4c3596e2b","execution":{"iopub.status.busy":"2021-12-17T15:53:11.230205Z","iopub.execute_input":"2021-12-17T15:53:11.23091Z","iopub.status.idle":"2021-12-17T15:53:11.237677Z","shell.execute_reply.started":"2021-12-17T15:53:11.230873Z","shell.execute_reply":"2021-12-17T15:53:11.236862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load the BERT model and pass a sample of batch data through:","metadata":{"id":"EixHDl20-1s7"}},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)","metadata":{"id":"rjxTYbWHuxbt","outputId":"94f22b4c-6eef-45f5-f8f3-5726447b3d63","execution":{"iopub.status.busy":"2021-12-17T15:53:13.671679Z","iopub.execute_input":"2021-12-17T15:53:13.672213Z","iopub.status.idle":"2021-12-17T15:53:27.871972Z","shell.execute_reply.started":"2021-12-17T15:53:13.672175Z","shell.execute_reply":"2021-12-17T15:53:27.871243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_batch = next(iter(DataLoader(train_dataset, batch_size=8, num_workers=2)))\nsample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape","metadata":{"id":"1CAT3QOy5NoC","outputId":"6ad9abee-9d70-43c2-9732-a8bb6bead138","execution":{"iopub.status.busy":"2021-12-17T15:53:32.514942Z","iopub.execute_input":"2021-12-17T15:53:32.515224Z","iopub.status.idle":"2021-12-17T15:53:32.662499Z","shell.execute_reply.started":"2021-12-17T15:53:32.515192Z","shell.execute_reply":"2021-12-17T15:53:32.661663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = bert_model(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])","metadata":{"id":"muXn8sWDuzrk","execution":{"iopub.status.busy":"2021-12-17T15:54:47.972859Z","iopub.execute_input":"2021-12-17T15:54:47.973205Z","iopub.status.idle":"2021-12-17T15:55:00.970476Z","shell.execute_reply.started":"2021-12-17T15:54:47.973155Z","shell.execute_reply":"2021-12-17T15:55:00.969718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.last_hidden_state.shape, output.pooler_output.shape","metadata":{"id":"lePNOTNuvrOb","outputId":"c65f3fd6-9d47-48fe-cfb4-25d79dc534ed","execution":{"iopub.status.busy":"2021-12-17T15:55:00.97219Z","iopub.execute_input":"2021-12-17T15:55:00.972531Z","iopub.status.idle":"2021-12-17T15:55:00.978273Z","shell.execute_reply.started":"2021-12-17T15:55:00.97249Z","shell.execute_reply":"2021-12-17T15:55:00.977598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `768` dimension comes from the BERT hidden size:","metadata":{"id":"pe4T07xLh5E6"}},{"cell_type":"code","source":"bert_model.config.hidden_size","metadata":{"id":"nqYGEVO1h553","outputId":"500febad-a44e-4f82-f805-0292329d097f","execution":{"iopub.status.busy":"2021-12-17T15:55:00.979709Z","iopub.execute_input":"2021-12-17T15:55:00.980188Z","iopub.status.idle":"2021-12-17T15:55:00.996843Z","shell.execute_reply.started":"2021-12-17T15:55:00.980151Z","shell.execute_reply":"2021-12-17T15:55:00.996032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The larger version of BERT has more attention heads and a larger hidden size.\n\nWe'll wrap our custom dataset into a [LightningDataModule](https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html):","metadata":{"id":"L1KGOf1CiEHM"}},{"cell_type":"code","source":"class ToxicCommentDataModule(pl.LightningDataModule):\n\n  def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n    super().__init__()\n    self.batch_size = batch_size\n    self.train_df = train_df\n    self.test_df = test_df\n    self.tokenizer = tokenizer\n    self.max_token_len = max_token_len\n\n  def setup(self, stage=None):\n    self.train_dataset = ToxicCommentsDataset(\n      self.train_df,\n      self.tokenizer,\n      self.max_token_len\n    )\n\n    self.test_dataset = ToxicCommentsDataset(\n      self.test_df,\n      self.tokenizer,\n      self.max_token_len\n    )\n\n  def train_dataloader(self):\n    return DataLoader(\n      self.train_dataset,\n      batch_size=self.batch_size,\n      shuffle=True,\n      num_workers=2\n    )\n\n  def val_dataloader(self):\n    return DataLoader(\n      self.test_dataset,\n      batch_size=self.batch_size,\n      num_workers=2\n    )\n\n  def test_dataloader(self):\n    return DataLoader(\n      self.test_dataset,\n      batch_size=self.batch_size,\n      num_workers=2\n    )","metadata":{"id":"_qLPXLkv-cwH","execution":{"iopub.status.busy":"2021-12-17T15:55:01.000935Z","iopub.execute_input":"2021-12-17T15:55:01.001169Z","iopub.status.idle":"2021-12-17T15:55:01.017371Z","shell.execute_reply.started":"2021-12-17T15:55:01.001139Z","shell.execute_reply":"2021-12-17T15:55:01.016339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`ToxicCommentDataModule` encapsulates all data loading logic and returns the necessary data loaders. Let's create an instance of our data module:","metadata":{"id":"SkF3aKCZkID0"}},{"cell_type":"code","source":"N_EPOCHS = 10\nBATCH_SIZE = 12\n\ndata_module = ToxicCommentDataModule(\n  train_df,\n  val_df,\n  tokenizer,\n  batch_size=BATCH_SIZE,\n  max_token_len=MAX_TOKEN_COUNT\n)","metadata":{"id":"ntTOVFoM3Nn-","execution":{"iopub.status.busy":"2021-12-17T15:55:01.022346Z","iopub.execute_input":"2021-12-17T15:55:01.023927Z","iopub.status.idle":"2021-12-17T15:55:01.032988Z","shell.execute_reply.started":"2021-12-17T15:55:01.023888Z","shell.execute_reply":"2021-12-17T15:55:01.032083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\nOur model will use a pre-trained [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and a linear layer to convert the BERT representation to a classification task. We'll pack everything in a [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html):\n","metadata":{"id":"rSQsvaBgoM7F"}},{"cell_type":"code","source":"class ToxicCommentTagger(pl.LightningModule):\n\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n    super().__init__()\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n    self.n_training_steps = n_training_steps\n    self.n_warmup_steps = n_warmup_steps\n    self.criterion = nn.BCELoss()\n\n  def forward(self, input_ids, attention_mask, labels=None):\n    output = self.bert(input_ids, attention_mask=attention_mask)\n    output = self.classifier(output.pooler_output)\n    output = torch.sigmoid(output)    \n    loss = 0\n    if labels is not None:\n        loss = self.criterion(output, labels)\n    return loss, output\n\n  def training_step(self, batch, batch_idx):\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    labels = batch[\"labels\"]\n    loss, outputs = self(input_ids, attention_mask, labels)\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n\n  def validation_step(self, batch, batch_idx):\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    labels = batch[\"labels\"]\n    loss, outputs = self(input_ids, attention_mask, labels)\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n    return loss\n\n  def test_step(self, batch, batch_idx):\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    labels = batch[\"labels\"]\n    loss, outputs = self(input_ids, attention_mask, labels)\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n    return loss\n\n  def training_epoch_end(self, outputs):\n    \n    labels = []\n    predictions = []\n    for output in outputs:\n      for out_labels in output[\"labels\"].detach().cpu():\n        labels.append(out_labels)\n      for out_predictions in output[\"predictions\"].detach().cpu():\n        predictions.append(out_predictions)\n\n    labels = torch.stack(labels).int()\n    predictions = torch.stack(predictions)\n\n    for i, name in enumerate(LABEL_COLUMNS):\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n\n\n  def configure_optimizers(self):\n\n    optimizer = AdamW(self.parameters(), lr=2e-5)\n\n    scheduler = get_linear_schedule_with_warmup(\n      optimizer,\n      num_warmup_steps=self.n_warmup_steps,\n      num_training_steps=self.n_training_steps\n    )\n\n    return dict(\n      optimizer=optimizer,\n      lr_scheduler=dict(\n        scheduler=scheduler,\n        interval='step'\n      )\n    )","metadata":{"id":"USKhIewsG3-k","execution":{"iopub.status.busy":"2021-12-17T15:55:01.036095Z","iopub.execute_input":"2021-12-17T15:55:01.036602Z","iopub.status.idle":"2021-12-17T15:55:01.084636Z","shell.execute_reply.started":"2021-12-17T15:55:01.036564Z","shell.execute_reply":"2021-12-17T15:55:01.083863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the implementation is just a boilerplate. Two points of interest are the way we configure the optimizers and calculating the area under ROC. We'll dive a bit deeper into those next.","metadata":{"id":"l7T5puRv5qBW"}},{"cell_type":"markdown","source":"### Optimizer scheduler\n\nThe job of a scheduler is to change the learning rate of the optimizer during training. This might lead to better performance of our model. We'll use the [get_linear_schedule_with_warmup](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup).\n\nLet's have a look at a simple example to make things clearer:","metadata":{"id":"CMcwEc3jokpX"}},{"cell_type":"code","source":"dummy_model = nn.Linear(2, 1)\n\noptimizer = AdamW(params=dummy_model.parameters(), lr=0.001)\n\nwarmup_steps = 20\ntotal_training_steps = 100\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer, \n  num_warmup_steps=warmup_steps,\n  num_training_steps=total_training_steps\n)\n\nlearning_rate_history = []\n\nfor step in range(total_training_steps):\n  optimizer.step()\n  scheduler.step()\n  learning_rate_history.append(optimizer.param_groups[0]['lr'])","metadata":{"id":"MnCP7YwNQKla","execution":{"iopub.status.busy":"2021-12-17T15:55:01.086309Z","iopub.execute_input":"2021-12-17T15:55:01.086747Z","iopub.status.idle":"2021-12-17T15:55:01.111207Z","shell.execute_reply.started":"2021-12-17T15:55:01.086712Z","shell.execute_reply":"2021-12-17T15:55:01.110564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(learning_rate_history, label=\"learning rate\")\nplt.axvline(x=warmup_steps, color=\"red\", linestyle=(0, (5, 10)), label=\"warmup end\")\nplt.legend()\nplt.xlabel(\"Step\")\nplt.ylabel(\"Learning rate\")\nplt.tight_layout();","metadata":{"id":"Lk94oy36QLe3","outputId":"8b2c2a41-f3fd-4b77-b33f-6103d6a8c8d1","execution":{"iopub.status.busy":"2021-12-17T15:55:01.11372Z","iopub.execute_input":"2021-12-17T15:55:01.11398Z","iopub.status.idle":"2021-12-17T15:55:01.603327Z","shell.execute_reply.started":"2021-12-17T15:55:01.113954Z","shell.execute_reply":"2021-12-17T15:55:01.602653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We simulate 100 training steps and tell the scheduler to warm up for the first 20. The learning rate grows to the initial fixed value of 0.001 during the warm-up and then goes down (linearly) to 0.\n\nTo use the scheduler, we need to calculate the number of training and warm-up steps. The number of training steps per epoch is equal to `number of training examples / batch size`. The number of total training steps is `training steps per epoch * number of epochs`:","metadata":{"id":"K4mFc9y2xA8S"}},{"cell_type":"code","source":"steps_per_epoch=len(train_df) // BATCH_SIZE\ntotal_training_steps = steps_per_epoch * N_EPOCHS","metadata":{"id":"r2ywIz841yni","execution":{"iopub.status.busy":"2021-12-17T15:55:01.604562Z","iopub.execute_input":"2021-12-17T15:55:01.604849Z","iopub.status.idle":"2021-12-17T15:55:01.609214Z","shell.execute_reply.started":"2021-12-17T15:55:01.604812Z","shell.execute_reply":"2021-12-17T15:55:01.608234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use a fifth of the training steps for a warm-up:","metadata":{"id":"YjV_biJ72ERg"}},{"cell_type":"code","source":"warmup_steps = total_training_steps // 5\nwarmup_steps, total_training_steps","metadata":{"id":"OSHJ3V47G90d","outputId":"7a42c77a-80fa-4bee-a981-1d9c2f993dcb","execution":{"iopub.status.busy":"2021-12-17T15:55:01.613078Z","iopub.execute_input":"2021-12-17T15:55:01.613314Z","iopub.status.idle":"2021-12-17T15:55:01.620736Z","shell.execute_reply.started":"2021-12-17T15:55:01.613283Z","shell.execute_reply":"2021-12-17T15:55:01.619939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now create an instance of our model:","metadata":{"id":"rBQ7EHkXNhz9"}},{"cell_type":"code","source":"model = ToxicCommentTagger(\n  n_classes=len(LABEL_COLUMNS),\n  n_warmup_steps=warmup_steps,\n  n_training_steps=total_training_steps \n)","metadata":{"id":"JObpLXJaZVPh","outputId":"92da6418-c40a-4ba5-babd-b5f59247c9f0","execution":{"iopub.status.busy":"2021-12-17T15:55:01.621827Z","iopub.execute_input":"2021-12-17T15:55:01.622078Z","iopub.status.idle":"2021-12-17T15:55:03.541648Z","shell.execute_reply.started":"2021-12-17T15:55:01.622044Z","shell.execute_reply":"2021-12-17T15:55:03.540793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation\n\nMulti-label classification boils down to doing binary classification for each label/tag.\n\nWe'll use Binary Cross Entropy to measure the error for each label. PyTorch has [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), which we're going to combine with a sigmoid function (as we did in the model implementation). Let's look at an example:","metadata":{"id":"oxFxZM3KTcKA"}},{"cell_type":"code","source":"criterion = nn.BCELoss()\n\nprediction = torch.FloatTensor(\n  [10.95873564, 1.07321467, 1.58524066, 0.03839076, 15.72987556, 1.09513213]\n)\nlabels = torch.FloatTensor(\n  [1., 0., 0., 0., 1., 0.]\n) ","metadata":{"id":"Z24SGvGggShC","execution":{"iopub.status.busy":"2021-12-17T15:55:03.543036Z","iopub.execute_input":"2021-12-17T15:55:03.543324Z","iopub.status.idle":"2021-12-17T15:55:03.548607Z","shell.execute_reply.started":"2021-12-17T15:55:03.543285Z","shell.execute_reply":"2021-12-17T15:55:03.54765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.sigmoid(prediction)","metadata":{"id":"S-4Hl3JWlTro","outputId":"1ee95f90-04cd-4117-de36-9499895e5373","execution":{"iopub.status.busy":"2021-12-17T15:55:03.550001Z","iopub.execute_input":"2021-12-17T15:55:03.550259Z","iopub.status.idle":"2021-12-17T15:55:03.561701Z","shell.execute_reply.started":"2021-12-17T15:55:03.550225Z","shell.execute_reply":"2021-12-17T15:55:03.560865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion(torch.sigmoid(prediction), labels)","metadata":{"id":"Pa-BDcFsgzLL","outputId":"dfad2a5b-9478-4ca2-d8b6-5a4a2cf0ecaf","execution":{"iopub.status.busy":"2021-12-17T15:55:03.56298Z","iopub.execute_input":"2021-12-17T15:55:03.563623Z","iopub.status.idle":"2021-12-17T15:55:03.575178Z","shell.execute_reply.started":"2021-12-17T15:55:03.563585Z","shell.execute_reply":"2021-12-17T15:55:03.574466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the same approach to calculate the loss of the predictions:","metadata":{"id":"fKNYepPeO6qh"}},{"cell_type":"code","source":"_, predictions = model(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])\npredictions","metadata":{"id":"7AwyolIxK9jz","outputId":"cf331d6b-cad3-484a-80ff-da1f28252851","execution":{"iopub.status.busy":"2021-12-17T15:55:03.576392Z","iopub.execute_input":"2021-12-17T15:55:03.576636Z","iopub.status.idle":"2021-12-17T15:55:13.542221Z","shell.execute_reply.started":"2021-12-17T15:55:03.576604Z","shell.execute_reply":"2021-12-17T15:55:13.541374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion(predictions, sample_batch[\"labels\"])","metadata":{"id":"hW-_f9XSa4k0","outputId":"944beffe-9112-43cc-d215-ea24c3e8e388","execution":{"iopub.status.busy":"2021-12-17T15:55:13.543552Z","iopub.execute_input":"2021-12-17T15:55:13.543894Z","iopub.status.idle":"2021-12-17T15:55:13.551261Z","shell.execute_reply.started":"2021-12-17T15:55:13.543858Z","shell.execute_reply":"2021-12-17T15:55:13.550418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC Curve\n\nAnother metric we're going to use is the area under the Receiver operating characteristic (ROC) for each tag. ROC is created by plotting the True Positive Rate (TPR) vs False Positive Rate (FPR):\n\n$$\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} \\text{+} \\text{FN}}\n$$\n\n$$\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} \\text{+} \\text{TN}}\n$$\n\n\n","metadata":{"id":"9ixjfLiimmK3"}},{"cell_type":"code","source":"from sklearn import metrics\n\nfpr = [0.        , 0.        , 0.        , 0.02857143, 0.02857143,\n       0.11428571, 0.11428571, 0.2       , 0.4       , 1.        ]\n\ntpr = [0.        , 0.01265823, 0.67202532, 0.76202532, 0.91468354,\n       0.97468354, 0.98734177, 0.98734177, 1.        , 1.        ]\n\n_, ax = plt.subplots()\nax.plot(fpr, tpr, label=\"ROC\")\nax.plot([0.05, 0.95], [0.05, 0.95], transform=ax.transAxes, label=\"Random classifier\", color=\"red\")\nax.legend(loc=4)\nax.set_xlabel(\"False positive rate\")\nax.set_ylabel(\"True positive rate\")\nax.set_title(\"Example ROC curve\")\nplt.show();","metadata":{"id":"34tCgIA7RPIb","outputId":"31e93834-00bc-417d-ac2c-b0ba9066c199","execution":{"iopub.status.busy":"2021-12-17T15:55:13.552914Z","iopub.execute_input":"2021-12-17T15:55:13.553196Z","iopub.status.idle":"2021-12-17T15:55:13.880618Z","shell.execute_reply.started":"2021-12-17T15:55:13.553162Z","shell.execute_reply":"2021-12-17T15:55:13.879958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"hFvqzK4Ua-16"}},{"cell_type":"code","source":"!rm -rf lightning_logs/\n!rm -rf checkpoints/","metadata":{"id":"l6YoDvFZeHRs","execution":{"iopub.status.busy":"2021-12-17T15:55:13.881663Z","iopub.execute_input":"2021-12-17T15:55:13.882041Z","iopub.status.idle":"2021-12-17T15:55:15.594116Z","shell.execute_reply.started":"2021-12-17T15:55:13.882004Z","shell.execute_reply":"2021-12-17T15:55:15.59318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir ./lightning_logs","metadata":{"id":"pVgeyKhaeGXZ","outputId":"62f7d344-cbc2-42d5-9975-0b62eb489398","execution":{"iopub.status.busy":"2021-12-17T15:55:15.596041Z","iopub.execute_input":"2021-12-17T15:55:15.596319Z","iopub.status.idle":"2021-12-17T15:55:21.823368Z","shell.execute_reply.started":"2021-12-17T15:55:15.596279Z","shell.execute_reply":"2021-12-17T15:55:21.822566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The beauty of PyTorch Lightning is that you can build a standard pipeline that you like and train (almost?) every model you might imagine. I prefer to use at least 3 components.\n\nCheckpointing that saves the best model (based on validation loss):","metadata":{"id":"ZRCh19djVBll"}},{"cell_type":"markdown","source":"Log the progress in TensorBoard:","metadata":{"id":"IgC4i7NjVPG2"}},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n  dirpath=\"checkpoints\",\n  filename=\"best-checkpoint\",\n  save_top_k=1,\n  verbose=True,\n  monitor=\"val_loss\",\n  mode=\"min\"\n)","metadata":{"id":"OX_FijnjVJ9u","execution":{"iopub.status.busy":"2021-12-17T15:55:21.825169Z","iopub.execute_input":"2021-12-17T15:55:21.825687Z","iopub.status.idle":"2021-12-17T15:55:21.832323Z","shell.execute_reply.started":"2021-12-17T15:55:21.825645Z","shell.execute_reply":"2021-12-17T15:55:21.83163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And early stopping triggers when the loss hasn't improved for the last 2 epochs (you might want to remove/reconsider this when training on real-world projects):","metadata":{"id":"4m8hnQ3VWKrQ"}},{"cell_type":"markdown","source":"We can start the training process:","metadata":{"id":"ScChaiTsWy90"}},{"cell_type":"code","source":"trainer = pl.Trainer(\n  max_epochs=N_EPOCHS,\n  gpus=1,\n  progress_bar_refresh_rate=30\n)","metadata":{"id":"lP7S4ulb3guC","outputId":"0b40720c-e719-4170-8d82-5e04c7b4751a","execution":{"iopub.status.busy":"2021-12-17T15:59:56.163532Z","iopub.execute_input":"2021-12-17T15:59:56.164135Z","iopub.status.idle":"2021-12-17T15:59:56.171409Z","shell.execute_reply.started":"2021-12-17T15:59:56.16409Z","shell.execute_reply":"2021-12-17T15:59:56.170623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, data_module)","metadata":{"id":"aUVqTCwn3rrQ","outputId":"691e021a-6f8b-4e6f-caba-c99fff55265f","execution":{"iopub.status.busy":"2021-12-17T16:00:00.070221Z","iopub.execute_input":"2021-12-17T16:00:00.070752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model improved for (only) 2 epochs. We'll have to evaluate it to see whether it is any good. Let's double-check the validation loss:","metadata":{"id":"l9KYHODAXFTH"}},{"cell_type":"code","source":"trainer.test()","metadata":{"id":"qAYbKDu4UR6M","outputId":"c0f88aa8-200b-447c-b235-f17e9c648cf4","execution":{"iopub.status.busy":"2021-12-17T15:55:22.287959Z","iopub.status.idle":"2021-12-17T15:55:22.288708Z","shell.execute_reply.started":"2021-12-17T15:55:22.288469Z","shell.execute_reply":"2021-12-17T15:55:22.288496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions\n\nI like to look at a small sample of predictions after the training is complete. This builds intuition about the quality of the predictions (qualitative evaluation).\n\nLet's load the best version (according to the validation loss) of our model:","metadata":{"id":"KKrlcyK_eX3s"}},{"cell_type":"code","source":"trained_model = ToxicCommentTagger.load_from_checkpoint(\n  trainer.checkpoint_callback.best_model_path,\n  n_classes=len(LABEL_COLUMNS)\n)\ntrained_model.eval()\ntrained_model.freeze()","metadata":{"id":"tGCXjnMPHlcp","execution":{"iopub.status.busy":"2021-12-17T15:55:22.289867Z","iopub.status.idle":"2021-12-17T15:55:22.290577Z","shell.execute_reply.started":"2021-12-17T15:55:22.290329Z","shell.execute_reply":"2021-12-17T15:55:22.290354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We put our model into \"eval\" mode, and we're ready to make some predictions. Here's the prediction on a sample (totally fictional) comment:","metadata":{"id":"gDCMgY1WyNf2"}},{"cell_type":"code","source":"test_comment = \"Hi, I'm Meredith and I'm an alch... good at supplier relations\"\n\nencoding = tokenizer.encode_plus(\n  test_comment,\n  add_special_tokens=True,\n  max_length=512,\n  return_token_type_ids=False,\n  padding=\"max_length\",\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n\n_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\ntest_prediction = test_prediction.flatten().numpy()\n\nfor label, prediction in zip(LABEL_COLUMNS, test_prediction):\n  print(f\"{label}: {prediction}\")","metadata":{"id":"-UIFiSLl6s8B","outputId":"55a7f953-9e9c-440e-95df-0a7b28fc93b9","execution":{"iopub.status.busy":"2021-12-17T15:55:22.291667Z","iopub.status.idle":"2021-12-17T15:55:22.29244Z","shell.execute_reply.started":"2021-12-17T15:55:22.292191Z","shell.execute_reply":"2021-12-17T15:55:22.292216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good. This one is pretty clean. We'll reduce the noise of the predictions by thresholding (0.5) them. We'll take only tag predictions above (or equal) to the threshold. Let's try something toxic:","metadata":{"id":"GTB254rv15vS"}},{"cell_type":"code","source":"THRESHOLD = 0.5\n\ntest_comment = \"You are such a loser! You'll regret everything you've done to me!\"\nencoding = tokenizer.encode_plus(\n  test_comment,\n  add_special_tokens=True,\n  max_length=512,\n  return_token_type_ids=False,\n  padding=\"max_length\",\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n\n_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\ntest_prediction = test_prediction.flatten().numpy()\n\nfor label, prediction in zip(LABEL_COLUMNS, test_prediction):\n  if prediction < THRESHOLD:\n    continue\n  print(f\"{label}: {prediction}\")","metadata":{"id":"iQBCGPM78o1m","outputId":"785c57cb-aa26-42b4-e835-238dbdad3599","execution":{"iopub.status.busy":"2021-12-17T15:55:22.293616Z","iopub.status.idle":"2021-12-17T15:55:22.29438Z","shell.execute_reply.started":"2021-12-17T15:55:22.294137Z","shell.execute_reply":"2021-12-17T15:55:22.294163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I definitely agree with those tags. It looks like our model is doing something reasonable, on those two examples.","metadata":{"id":"oXSHR12g4JCX"}},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"8O1pKXAqD-KL"}},{"cell_type":"markdown","source":"Let's get a more complete overview of the performance of our model. We'll start by taking all predictions and labels from the validation set:","metadata":{"id":"oF5KB8wI7JOS"}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrained_model = trained_model.to(device)\n\nval_dataset = ToxicCommentsDataset(\n  val_df,\n  tokenizer,\n  max_token_len=MAX_TOKEN_COUNT\n)\n\npredictions = []\nlabels = []\n\nfor item in tqdm(val_dataset):\n  _, prediction = trained_model(\n    item[\"input_ids\"].unsqueeze(dim=0).to(device), \n    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n  )\n  predictions.append(prediction.flatten())\n  labels.append(item[\"labels\"].int())\n\npredictions = torch.stack(predictions).detach().cpu()\nlabels = torch.stack(labels).detach().cpu()","metadata":{"id":"pUp4LKr282sf","outputId":"5821b221-e431-4b24-9644-74ef4c6e5d0c","execution":{"iopub.status.busy":"2021-12-17T15:55:22.295565Z","iopub.status.idle":"2021-12-17T15:55:22.296342Z","shell.execute_reply.started":"2021-12-17T15:55:22.296096Z","shell.execute_reply":"2021-12-17T15:55:22.296122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One simple metric is the accuracy of the model:","metadata":{"id":"Xsv9iIee7tKU"}},{"cell_type":"code","source":"accuracy(predictions, labels, threshold=THRESHOLD)","metadata":{"id":"WGkvsx4h_lK9","outputId":"d6af58db-e6ba-4cf8-ec08-aa024734aee1","execution":{"iopub.status.busy":"2021-12-17T15:55:22.298686Z","iopub.status.idle":"2021-12-17T15:55:22.299095Z","shell.execute_reply.started":"2021-12-17T15:55:22.298878Z","shell.execute_reply":"2021-12-17T15:55:22.2989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's great, but you should take this result with a grain of salt. We have a very imbalanced dataset. Let's check the ROC for each tag:","metadata":{"id":"jnG4MuYo8aTU"}},{"cell_type":"code","source":"print(\"AUROC per tag\")\nfor i, name in enumerate(LABEL_COLUMNS):\n  tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n  print(f\"{name}: {tag_auroc}\")","metadata":{"id":"IV5jhrAyBehY","outputId":"f56ae7ab-04ab-45b1-c34b-596a0d628b2d","execution":{"iopub.status.busy":"2021-12-17T15:55:22.300108Z","iopub.status.idle":"2021-12-17T15:55:22.300842Z","shell.execute_reply.started":"2021-12-17T15:55:22.300576Z","shell.execute_reply":"2021-12-17T15:55:22.300607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very good results, but just before we go party, let's check the classification report for each class. To make this work, we must apply thresholding to the predictions:","metadata":{"id":"tsZMX6mEbooM"}},{"cell_type":"code","source":"y_pred = predictions.numpy()\ny_true = labels.numpy()\n\nupper, lower = 1, 0\n\ny_pred = np.where(y_pred > THRESHOLD, upper, lower)\n\nprint(classification_report(\n  y_true, \n  y_pred, \n  target_names=LABEL_COLUMNS, \n  zero_division=0\n))","metadata":{"id":"0muhafskBrYH","outputId":"30614e61-ba70-4d32-8a4b-d901f0606ef4","execution":{"iopub.status.busy":"2021-12-17T15:55:22.301886Z","iopub.status.idle":"2021-12-17T15:55:22.302289Z","shell.execute_reply.started":"2021-12-17T15:55:22.302062Z","shell.execute_reply":"2021-12-17T15:55:22.302083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That gives us a much more realistic picture of the overall performance. The model makes mistakes on the tags will low amounts of examples. What can you do about it?","metadata":{"id":"6W2FNRmKeeV1"}},{"cell_type":"markdown","source":"## Summary\n\nGreat job, you have a model that can tell (to some extent) if a text is toxic (and what kind) or not! Fine-tuning modern pre-trained Transformer models allow you to get high accuracy on a variety of NLP tasks with little compute power and small datasets.\n\n- [Read the tutorial](https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/)\n- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing)\n- [Read the *Getting Things Done with Pytorch* book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n\nIn this tutorial, you'll learned how to:\n\n- Load, balance and split text data into sets\n- Tokenize text (with BERT tokenizer) and create PyTorch dataset\n- Fine-tune BERT model with PyTorch Lightning\n- Find out about warmup steps and use a learning rate scheduler\n- Use area under the ROC and binary cross-entropy to evaluate the model during training\n- How to make predictions using the fine-tuned BERT model\n- Evaluate the performance of the model for each class (possible comment tag)\n\nCan you increase the accuracy of the model? How about better parameters or different learning rate scheduling? Let me know in the comments.\n\n","metadata":{"id":"_qkmHh9SRfPd"}},{"cell_type":"markdown","source":"## References\n\n- [Toxic comments EDA](https://www.kaggle.com/swathi314/toxic-comment-classification-eda)\n- [Receiver operating characteristic on ML crash course](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)","metadata":{"id":"iPd8y3OZqK_O"}}]}